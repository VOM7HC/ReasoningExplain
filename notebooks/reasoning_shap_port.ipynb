{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1da99666",
   "metadata": {},
   "source": [
    "# Summary\n",
    "- Ports `src/reasoning_shap.py` into an interactive workflow so you can inspect reasoning-step SHAP analysis directly in Jupyter while experimenting with prompts and models.\n",
    "\n",
    "# Inputs\n",
    "- Problem statement text for the reasoning task.\n",
    "- Optional list of `ReasoningStep` objects or an Ollama endpoint for automatic step generation.\n",
    "- Model/vectorizer selection (`OllamaModel`, embeddings, or the dummy demo model provided here).\n",
    "\n",
    "# Outputs\n",
    "- `pandas.DataFrame` of reasoning-step coalitions with similarity scores.\n",
    "- Bar chart of reasoning-step importance values.\n",
    "- Color-highlighted reasoning narrative that emphasizes high-importance sentences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2219af",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys\n",
    "import math\n",
    "import hashlib\n",
    "from typing import List, Optional\n",
    "import re\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors as mcolors\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "SRC_PATH = PROJECT_ROOT / 'src'\n",
    "if str(SRC_PATH) not in sys.path:\n",
    "    sys.path.append(str(SRC_PATH))\n",
    "\n",
    "plt.style.use('seaborn-v0_8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63decb55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from reasoning_shap import ReasoningSHAP, ReasoningStep\n",
    "from base import ModelBase, TfidfTextVectorizer, EmbeddingVectorizer, OllamaModel\n",
    "\n",
    "\n",
    "class DummyReasoningModel(ModelBase):\n",
    "    '''Deterministic stand-in for Ollama that scores prompts by coverage.'''\n",
    "\n",
    "    def __init__(self, expected_steps: Optional[int] = None, noise_scale: float = 0.05):\n",
    "        super().__init__(model_name='dummy-reasoning-model')\n",
    "        self.expected_steps = expected_steps\n",
    "        self.noise_scale = noise_scale\n",
    "\n",
    "    def generate(self, prompt: str) -> str:\n",
    "        total_steps = self.expected_steps if self.expected_steps is not None else max(prompt.count('Step '), 1)\n",
    "        present_steps = prompt.count('Step ')\n",
    "        coverage = present_steps / max(total_steps, 1)\n",
    "        digest = hashlib.sha1(prompt.encode('utf-8')).hexdigest()\n",
    "        jitter = ((int(digest[:6], 16) % 2000) / 1000 - 1.0) * self.noise_scale\n",
    "        score = float(np.clip(coverage + jitter, 0.0, 1.0))\n",
    "        return (\n",
    "            f\"Confidence Score: {score:.3f}\n",
    "\"\n",
    "            f\"Steps Included: {present_steps}/{total_steps}\n",
    "\"\n",
    "            'Reasoning Summary:\n",
    "'\n",
    "            f\"{prompt}\"\n",
    "        )\n",
    "\n",
    "\n",
    "class NotebookReasoningSHAP(ReasoningSHAP):\n",
    "    '''ReasoningSHAP variant with a lightweight fallback attribution routine.'''\n",
    "\n",
    "    def _calculate_shapley_values(self, df: pd.DataFrame, content):\n",
    "        try:\n",
    "            return super()._calculate_shapley_values(df, content)\n",
    "        except (ModuleNotFoundError, ImportError) as exc:\n",
    "            self._debug_print(f\"Falling back to simplified attribution because {exc.__class__.__name__}: {exc}\")\n",
    "            return self._fallback_shapley_values(df, content)\n",
    "\n",
    "    def _fallback_shapley_values(self, df: pd.DataFrame, content) -> dict:\n",
    "        steps = self._get_samples(content)\n",
    "        if not steps:\n",
    "            return {}\n",
    "\n",
    "        def normalize_indexes(indexes):\n",
    "            if indexes is None:\n",
    "                return tuple()\n",
    "            if isinstance(indexes, tuple):\n",
    "                return indexes\n",
    "            if isinstance(indexes, list):\n",
    "                return tuple(indexes)\n",
    "            if hasattr(indexes, 'tolist'):\n",
    "                return tuple(indexes.tolist())\n",
    "            return tuple(indexes)\n",
    "\n",
    "        contributions = {}\n",
    "        for idx, step in enumerate(steps):\n",
    "            target = idx + 1\n",
    "            contain = df[df['Indexes'].apply(lambda ids, t=target: t in normalize_indexes(ids))]\n",
    "            without = df[df['Indexes'].apply(lambda ids, t=target: t not in normalize_indexes(ids))]\n",
    "            contain_score = float(contain['Similarity'].mean()) if not contain.empty else 0.0\n",
    "            without_score = float(without['Similarity'].mean()) if not without.empty else 0.0\n",
    "            contributions[f\"{step}_{idx + 1}\"] = contain_score - without_score\n",
    "\n",
    "        min_value = min(contributions.values())\n",
    "        shifted = {k: v - min_value for k, v in contributions.items()}\n",
    "        total = sum(shifted.values())\n",
    "        if total <= 0:\n",
    "            uniform = 1.0 / max(len(shifted), 1)\n",
    "            return {k: uniform for k in shifted}\n",
    "        return {k: v / total for k, v in shifted.items()}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "017ab77f",
   "metadata": {},
   "source": [
    "## Configure the analyzer\n",
    "Toggle `USE_OLLAMA` to switch between the dummy demo pipeline and the full Ollama-backed workflow.  \\\n",
    "Running the full pipeline requires `optuna`, `shap`, and `xgboost` plus an accessible Ollama endpoint.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0b9281c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "USE_OLLAMA = False  # set to True for the full Phi-4 reasoning pipeline\n",
    "\n",
    "problem = '''\n",
    "A train leaves Station A at 9:00 AM traveling at 60 mph toward Station B.\n",
    "Another train leaves Station B at 10:00 AM traveling at 80 mph toward Station A.\n",
    "The distance between the stations is 280 miles.\n",
    "At what time will the trains meet?\n",
    "'''.strip()\n",
    "\n",
    "manual_steps: List[ReasoningStep] = [\n",
    "    ReasoningStep(\n",
    "        step_number=1,\n",
    "        description='Combine train speeds',\n",
    "        content=(\n",
    "            'When two trains head toward each other, their individual speeds add. '\n",
    "            'Add 60 mph and 80 mph to obtain a combined closing speed of 140 mph.'\n",
    "        ),\n",
    "    ),\n",
    "    ReasoningStep(\n",
    "        step_number=2,\n",
    "        description='Relate distance and time',\n",
    "        content=(\n",
    "            'The distance between the stations is 280 miles. '\n",
    "            'Time to meet equals distance divided by closing speed.'\n",
    "        ),\n",
    "    ),\n",
    "    ReasoningStep(\n",
    "        step_number=3,\n",
    "        description='Compute meeting interval',\n",
    "        content=(\n",
    "            'Divide 280 miles by 140 mph to get 2 hours. '\n",
    "            'This interval is measured from the moment the first train departs Station A.'\n",
    "        ),\n",
    "    ),\n",
    "    ReasoningStep(\n",
    "        step_number=4,\n",
    "        description='Account for staggered departures',\n",
    "        content=(\n",
    "            'Train B leaves one hour after Train A. '\n",
    "            'Subtract the one-hour head start to see that Train B travels for only 1 hour before meeting.'\n",
    "        ),\n",
    "    ),\n",
    "    ReasoningStep(\n",
    "        step_number=5,\n",
    "        description='State the meeting time',\n",
    "        content=(\n",
    "            'Train A has been on the track for 2 hours, so the trains meet at 11:00 AM. '\n",
    "            'Train B has been moving for 1 hour and arrives at the same moment.'\n",
    "        ),\n",
    "    ),\n",
    "]\n",
    "\n",
    "if USE_OLLAMA:\n",
    "    api_url = os.getenv('API_URL', 'http://localhost:11434')\n",
    "    model_name = os.getenv('OLLAMA_MODEL', 'phi4-reasoning:latest')\n",
    "    model = OllamaModel(model_name=model_name, api_url=api_url)\n",
    "    vectorizer = EmbeddingVectorizer(model_name=os.getenv('EMBEDDING_MODEL', 'all-MiniLM-L6-v2'))\n",
    "    analyzer = NotebookReasoningSHAP(model=model, vectorizer=vectorizer, debug=False)\n",
    "else:\n",
    "    model = DummyReasoningModel(expected_steps=len(manual_steps))\n",
    "    vectorizer = TfidfTextVectorizer()\n",
    "    analyzer = NotebookReasoningSHAP(model=model, vectorizer=vectorizer, debug=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad802b31",
   "metadata": {},
   "outputs": [],
   "source": [
    "if USE_OLLAMA:\n",
    "    results_df = analyzer.analyze_reasoning(\n",
    "        problem=problem,\n",
    "        steps=None,\n",
    "        auto_generate_steps=True,\n",
    "        num_steps=5,\n",
    "        sampling_ratio=0.3,\n",
    "        max_combinations=80,\n",
    "    )\n",
    "else:\n",
    "    results_df = analyzer.analyze_reasoning(\n",
    "        problem=problem,\n",
    "        steps=manual_steps,\n",
    "        auto_generate_steps=False,\n",
    "        sampling_ratio=0.4,\n",
    "        max_combinations=32,\n",
    "    )\n",
    "\n",
    "results_df.sort_values('Similarity', ascending=False).head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a10635",
   "metadata": {},
   "outputs": [],
   "source": [
    "analyzer.plot_importance()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2a6a769",
   "metadata": {},
   "outputs": [],
   "source": [
    "def render_colored_reasoning(analyzer: NotebookReasoningSHAP, top_n: Optional[int] = None) -> None:\n",
    "    shap_vals = analyzer.shapley_values or {}\n",
    "    if not shap_vals:\n",
    "        display(HTML('<p>No Shapley values available to visualize.</p>'))\n",
    "        return\n",
    "\n",
    "    steps = analyzer.reasoning_steps\n",
    "    cmap = plt.cm.YlOrRd\n",
    "    sorted_items = sorted(shap_vals.items(), key=lambda item: item[1], reverse=True)\n",
    "    if top_n is not None:\n",
    "        sorted_items = sorted_items[:top_n]\n",
    "\n",
    "    values = [value for _, value in sorted_items]\n",
    "    min_val, max_val = min(values), max(values)\n",
    "\n",
    "    def scale(value: float) -> float:\n",
    "        if math.isclose(max_val, min_val):\n",
    "            return 0.7\n",
    "        return (value - min_val) / (max_val - min_val)\n",
    "\n",
    "    html_parts = []\n",
    "    for key, value in sorted_items:\n",
    "        match = re.search(r'(\\d+)', key)\n",
    "        if not match:\n",
    "            continue\n",
    "        step_num = int(match.group(1))\n",
    "        step = next((s for s in steps if s.step_number == step_num), None)\n",
    "        if step is None:\n",
    "            continue\n",
    "        color = mcolors.to_hex(cmap(0.35 + 0.5 * scale(value)))\n",
    "        sentences = [s.strip() for s in re.split(r'(?<=[.!?])\\s+', step.content) if s.strip()]\n",
    "        sentence_html = ' '.join(\n",
    "            f\"<span style='background-color:{color}; padding:2px 4px; border-radius:4px;'>{sentence}</span>\"\n",
    "            for sentence in sentences\n",
    "        )\n",
    "        html_parts.append(\n",
    "            f\"<p><strong>Step {step.step_number}: {step.description}</strong> — importance {value:.2%}<br>{sentence_html}</p>\"\n",
    "        )\n",
    "\n",
    "    display(HTML(''.join(html_parts)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30cc9d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_colored_reasoning(analyzer)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
